AWSTemplateFormatVersion: '2010-09-09'
Description: Creates a Bedrock Agent and action group

Parameters:
  BedrockModelId:
    Type: String
    Description: The ID of the Foundation Model to use for the Agent
    Default: anthropic.claude-3-sonnet-20240229-v1:0
  EnvironmentName:
    Type: String
    Description: The name of the agent environment, used to differenciate agent application, name must be lowercase.
    Default: research-assistant
  GitRepoURL:
    Type: String
    Default: https://github.com/zeekgaws/biomarker-agent.git
    Description: Git repository URL where the code files are stored
  ImageTag:
    Type: String
    Default: latest
    Description: Tag for the Docker image


Resources:
  # Create ECR repository
  ECRRepository:
    Type: AWS::ECR::Repository
    # DependsOn: CleanupCustomResource
    Properties:
      RepositoryName: lifelines-lambda-sample

  EnsureECRImagePushed:
    Type: Custom::EnsureECRImagePushed
    DependsOn: 
      - TriggerBuildCustomResource
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ECRRepository: !Ref ECRRepository
      ImageTag: !Ref ImageTag
      

  CleanupLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt CleanupLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          
          def handler(event, context):
            if event['RequestType'] == 'Delete':
              try:
                # Clean up ECR
                ecr = boto3.client('ecr')
                repository_name = event['ResourceProperties']['ECRRepositoryName']
                images = ecr.list_images(repositoryName=repository_name)
                if images['imageIds']:
                  ecr.batch_delete_image(
                    repositoryName=repository_name,
                    imageIds=images['imageIds']
                  )
                
                # Clean up S3
                s3 = boto3.resource('s3')
                bucket = s3.Bucket(event['ResourceProperties']['S3BucketName'])
                bucket.objects.all().delete()
                
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                print(e)
                cfnresponse.send(event, context, cfnresponse.FAILED, {})
            else:
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Runtime: python3.8
      Timeout: 300

  CleanupLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CleanupPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:DeleteObject'
                  - 'ecr:ListImages'
                  - 'ecr:BatchDeleteImage'
                Resource: '*'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  CleanupCustomResource:
    Type: Custom::Cleanup
    DependsOn: 
      - S3Bucket
      - ECRRepository
    Properties:
      ServiceToken: !GetAtt CleanupLambdaFunction.Arn
      ECRRepositoryName: !Ref ECRRepository
      S3BucketName: !Ref S3Bucket
  
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${EnvironmentName}-${AWS::AccountId}-agent-build-bucket'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

  # Create CloudWatch Log Group
  CodeBuildLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/codebuild/${AWS::StackName}-DockerPushProject'
      RetentionInDays: 14

  # Log into ECR and push Docker image
  DockerPushLogic:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: DockerPushProject
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: S3
        Location: !Ref S3Bucket
        Name: pubmed-lambda-function.zip
        NamespaceType: NONE
        Packaging: ZIP
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref CodeBuildLogGroup
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        PrivilegedMode: true
      Source:
        Type: GITHUB
        Location: !Ref GitRepoURL
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - echo "Starting pre_build phase"
                - echo "Logging into Amazon ECR..."
                - aws ecr get-login-password --region ${AWS::Region} | docker login --username AWS --password-stdin ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
                - echo "ECR login complete"
            build:
              commands:
                - echo "Starting build phase"
                - echo "Cloning Git repository..."
                - git clone ${GitRepoURL}
                - cd biomarker-agent/scientific-plots-with-lifelines
                - echo "Building Docker image..."
                - docker build -t lifelines-python3.12-v2 .
                - echo "Tagging Docker image..."
                - docker tag lifelines-python3.12-v2:latest ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
                - echo "Docker image tagged"
                - cd ../..
                - echo "Zipping Lambda function..."
                - cd biomarker-agent/pubmed-lambda-function
                - zip -r ../../pubmed-lambda-function.zip .
                - cd ../..
                - echo "Lambda function zipped"
            post_build:
              commands:
                - echo "Starting post_build phase"
                - echo "Pushing Docker image to ECR..."
                - docker push ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
                - echo "Docker image pushed to ECR"
                - echo "Displaying Docker images"
                - docker images
                - echo "Displaying ECR repository contents"
                - aws ecr list-images --repository-name ${ECRRepository}
          artifacts:
            files:
              - pubmed-lambda-function.zip
      TimeoutInMinutes: 10

  # IAM role for CodeBuild
  CodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser'
        - 'arn:aws:iam::aws:policy/CloudWatchLogsFullAccess'
      Policies:
        - PolicyName: CodeBuildBasePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
              - Effect: Allow
                Resource: 
                  - !Sub 'arn:aws:s3:::${AWS::AccountId}-codepipeline-${AWS::Region}'
                  - !Sub 'arn:aws:s3:::${AWS::AccountId}-codepipeline-${AWS::Region}/*'
                Action:
                  - 's3:GetObject'
                  - 's3:GetObjectVersion'
                  - 's3:GetBucketVersioning'
                  - 's3:PutObject'
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'ecr:GetAuthorizationToken'
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:ListBucket
                Resource: 
                  - !GetAtt S3Bucket.Arn
                  - !Sub "${S3Bucket.Arn}/*"
        - PolicyName: ECRAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ecr:GetAuthorizationToken
                  - ecr:BatchCheckLayerAvailability
                  - ecr:GetDownloadUrlForLayer
                  - ecr:GetRepositoryPolicy
                  - ecr:DescribeRepositories
                  - ecr:ListImages
                  - ecr:DescribeImages
                  - ecr:BatchGetImage
                  - ecr:InitiateLayerUpload
                  - ecr:UploadLayerPart
                  - ecr:CompleteLayerUpload
                  - ecr:PutImage
                Resource: '*'

  TriggerBuildLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt TriggerBuildLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import time

          def handler(event, context):
              if event['RequestType'] in ['Create', 'Update']:
                  try:
                      codebuild = boto3.client('codebuild')
                      ecr = boto3.client('ecr')
                      
                      if 'ProjectName' in event['ResourceProperties']:
                          # This is for TriggerBuildCustomResource
                          project_name = event['ResourceProperties']['ProjectName']
                          response = codebuild.start_build(projectName=project_name)
                          build_id = response['build']['id']
                          print(f"Build started: {build_id}")
                          
                          # Wait for build to complete
                          while True:
                              build_status = codebuild.batch_get_builds(ids=[build_id])['builds'][0]['buildStatus']
                              if build_status == 'SUCCEEDED':
                                  print("Build completed successfully")
                                  break
                              elif build_status in ['FAILED', 'STOPPED', 'TIMED_OUT']:
                                  print(f"Build failed with status: {build_status}")
                                  cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": f"Build failed with status: {build_status}"})
                                  return
                              time.sleep(10)  # Wait for 10 seconds before checking again
                          
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {"BuildId": build_id})
                          
                      elif 'ECRRepository' in event['ResourceProperties']:
                          # This is for EnsureECRImagePushed
                          repository_name = event['ResourceProperties']['ECRRepository']
                          image_tag = event['ResourceProperties']['ImageTag']
                          
                          # Wait for image to be available in ECR
                          max_attempts = 30  # Maximum number of attempts
                          for attempt in range(max_attempts):
                              try:
                                  ecr.describe_images(repositoryName=repository_name, imageIds=[{'imageTag': image_tag}])
                                  print(f"Image {repository_name}:{image_tag} exists in ECR")
                                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                                  return
                              except ecr.exceptions.ImageNotFoundException:
                                  if attempt == max_attempts - 1:
                                      print(f"Image {repository_name}:{image_tag} not found in ECR after {max_attempts} attempts")
                                      cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": "Image not found in ECR after maximum attempts"})
                                      return
                                  time.sleep(10)  # Wait for 10 seconds before trying again
                      else:
                          cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": "Invalid ResourceProperties"})
                  except Exception as e:
                      print(f"Error: {str(e)}")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})
              else:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Runtime: python3.8
      Timeout: 900

  TriggerBuildLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CodeBuildStartBuildPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 
                  - codebuild:StartBuild
                  - codebuild:BatchGetBuilds
                Resource: !GetAtt DockerPushLogic.Arn
        - PolicyName: CloudWatchLogsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
        - PolicyName: ECRDescribeImagesPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: ecr:DescribeImages
                Resource: '*'
  
  
  TriggerBuildCustomResource:
    Type: Custom::TriggerBuild
    DependsOn: DockerPushLogic
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ProjectName: !Ref DockerPushLogic


  AgentResource:
    Type: AWS::Bedrock::Agent
    Properties:
      AgentName: !Sub biomarker-agent-${EnvironmentName}
      AgentResourceRoleArn: !GetAtt AgentRole.Arn
      AutoPrepare : True
      FoundationModel: !Ref BedrockModelId
      Instruction: |
        You are a medical research assistant that generates SQL queries against
        a database containing medical biomarker information. You can look up the
        schema of the database tables by calling the `/getschema` API to ensure
        your queries are correctly formatted and targeting the appropriate columns.
        Before executing any SQL query, you should first evaluate it with the rationale
        of the specific step  by calling the `/evaluatesql` API with the proposed SQL query
        and the rationale of the specific step as the question. This API will analyze the query
        and return an evaluated or optimized version if needed. Only after receiving the evaluated
        query from this API should you proceed to execute the query by calling the `/queryredshift`
        API. Please remove all next line ("\n") characters from the sql queries. You can plot a
        Kaplan Meier chart by calling the '/plot_kaplan_meier' API. To invoke the '/plot_kaplan_meier'
        API, you can generate the parameter values by executing python code by mapping survival status
        with value 0 for Alive and value 1 for Dead as the event parameter, time to death(days) as the
        duration parameter, and expression group with value 0 as the filter value for baseline parameter
        and expression group with value 1 as the filter value for condition parameter. Use the
        '/fit_survival_regression' API to identify a best performing biomarker based on the p-value
        summary by fitting a survival regression model. To invoke  '/fit_survival_regression' API, you
        must retrieve all the records with columns that include survival status as first column, then
        duration and  then the required biomarkers. You can process computed tomographic (CT) lung
        imaging biomarker with the 'compute_imaging_biomarker' API. You should identify patient
        subject ID based on the conversation and pass it as an array of string such as ["R01-043", "R01-93"].
        You have access to tools and APIs to evaluate SQL, query redshift database, plot a kaplan meier chart,
        fit a survival regression model and compute imaging biomarkers. If required, you have access to code
        interpreter to execute python code, and can write code as needed and pass it to code interpreter to execute.


      Description: "Agent for querying biomaker database."
      ActionGroups:
        - ActionGroupName: sqlActionGroup
          Description: Action for getting the database schema and querying the database
          ActionGroupExecutor: 
            Lambda: !GetAtt AgentLambdaFunction.Arn
          ApiSchema:
            Payload: |
              {
                  "openapi": "3.0.1",
                  "info": {
                      "title": "Database schema look up and query APIs",
                      "version": "1.0.0",
                      "description": "APIs for looking up database table schemas and making queries to database tables."
                  },
                  "paths": {
                      "/getschema": {
                          "get": {
                              "summary": "Get a list of all columns in the redshift database",
                              "description": "Get the list of all columns in the redshift database table. Return all the column information in database table.",
                              "operationId": "getschema",
                              "responses": {
                                  "200": {
                                      "description": "Gets the list of table names and their schemas in the database",
                                      "content": {
                                          "application/json": {
                                              "schema": {
                                                  "type": "array",
                                                  "items": {
                                                      "type": "object",
                                                      "properties": {
                                                          "Table": {
                                                              "type": "string",
                                                              "description": "The name of the table in the database."
                                                          },
                                                          "Schema": {
                                                              "type": "string",
                                                              "description": "The schema of the table in the database. Contains all columns needed for making queries."
                                                          }
                                                      }
                                                  }
                                              }
                                          }
                                      }
                                  }
                              }
                          }
                      },
                      "/queryredshift": {
                          "post": {
                              "summary": "API to send query to the redshift database table",
                              "description": "Send a query to the database table to retrieve information pertaining to the users question . The API takes in only one SQL query at a time, sends the SQL statement and returns the query results from the table. This API should be called for each SQL query to a database table.",
                              "operationId": "queryredshift",
                              "requestBody": {
                                  "required": true,
                                  "content": {
                                      "application/json": {
                                          "schema": {
                                              "type": "object",
                                              "properties": {
                                                  "query": {
                                                      "type": "string",
                                                      "description": "SQL statement to query database table."
                                                  }
                                              },
                                              "required": [
                                                  "query"
                                              ]
                                          }
                                      }
                                  }
                              },
                              "responses": {
                                  "200": {
                                      "description": "Query sent successfully",
                                      "content": {
                                          "application/json": {
                                              "schema": {
                                                  "type": "object",
                                                  "properties": {
                                                      "responsebody": {
                                                          "type": "string",
                                                          "description": "The query response from the database."
                                                      }
                                                  }
                                              }
                                          }
                                      }
                                  },
                                  "400": {
                                      "description": "Bad request. One or more required fields are missing or invalid."
                                  }
                              }
                          }
                      },
                      "/refineesql": {
                          "post": {
                            "summary": "Evaluate SQL query efficiency",
                            "description": "Evaluate the efficiency of an SQL query based on the provided schema, query, and question.",
                            "operationId": "evaluatesql",
                            "requestBody": {
                              "required": true,
                              "content": {
                                "application/json": {
                                  "schema": {
                                    "type": "object",
                                    "properties": {
                                      "sql": {
                                        "type": "string",
                                        "description": "The SQL query to evaluate."
                                      },
                                      "question": {
                                        "type": "string",
                                        "description": "The question related to the rationale of the specific step."
                                      }
                                    },
                                    "required": [
                                      "sql",
                                      "question"
                                    ]
                                  }
                                }
                              }
                            },
                            "responses": {
                              "200": {
                                "description": "Successful response",
                                "content": {
                                  "application/json": {
                                    "schema": {
                                      "type": "object",
                                      "properties": {
                                        "evaluatedQuery": {
                                          "type": "string",
                                          "description": "The evaluated SQL query, or the original query if it is efficient."
                                        }
                                      }
                                    }
                                  }
                                }
                              },
                              "400": {
                                "description": "Bad request. One or more required fields are missing or invalid."
                              }
                            }
                          }
                        }
                      }
                    }
        - ActionGroupName: scientificAnalysisActionGroup
          Description: Actions for scientific analysis with lifelines library 
          ActionGroupExecutor: 
            Lambda: !GetAtt ScientificPlotLambdaFunction.Arn
          FunctionSchema:
            Functions:
              - Description: "Plots a Kaplan-Meier survival curve"
                Name: "plot_kaplan_meier"
                Parameters:
                  biomarker_name:
                    Type: "string"
                    Description: "name of the biomarker"
                    Required: true
                  duration_baseline:
                    Type: "array"
                    Description: "duration in number of days for baseline"
                    Required: true
                  duration_condition:
                    Type: "array"
                    Description: "duration in number of days for condition"
                    Required: true
                  event_baseline:
                    Type: "array"
                    Description: "survival event for baseline"
                    Required: true
                  event_condition:
                    Type: "array"
                    Description: "survival event for condition"
                    Required: true
              - Description: "Fits a survival regression model"
                Name: "fit_survival_regression"
                Parameters:
                  bucket:
                    Type: "string"
                    Description: "s3 bucket"
                    Required: true
                  key:
                    Type: "string"
                    Description: "object key"
                    Required: true
          
        - ActionGroupName: queryPubMed
          Description: Actions for fetching biomedical literature from PubMed
          ActionGroupExecutor: 
            Lambda: !GetAtt QueryPubMedLambdaFunction.Arn
          ApiSchema:
            Payload: |
                {
                    "openapi": "3.0.0",
                    "info": {
                        "title": "fetch biomedical literature",
                        "version": "1.0.0",
                        "description": "PubMed API to help answer users question using abstracts from biomedical literature."
                    },
                    "paths": {
                        "/query-pubmed": {
                            "post": {
                                "summary": "Query pubmed to relevant information from abstracts of biomedical articles.",
                                "description": "Query pubmed to relevant information from abstracts of biomedical articles. The PubMed API takes in the user query then returns the abstracts of top 5 relevant articles.",
                                "operationId": "query-pubmed",
                                "parameters": [
                                    {
                                        "name": "query",
                                        "in": "query",
                                        "description": "user query",
                                        "required": true,
                                        "schema": {
                                            "type": "string"
                                        }
                                    }
                                ],                
                                "responses": {
                                    "200": {
                                        "description": "Query pubmed to relevant information from abstracts of biomedical articles.",
                                        "content": {
                                            "application/json": {
                                                "schema": {
                                                    "type": "object",
                                                    "properties": {
                                                        "answer": {
                                                            "type": "string",
                                                            "description": "The response to user query with list of pubmed article abstracts."
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }


  
  AgentAliasResource:
    Type: AWS::Bedrock::AgentAlias
    Properties:
      AgentId: !GetAtt AgentResource.AgentId
      AgentAliasName: !Sub biomakers-alias
      

  AgentRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub AmazonBedrockExecutionRoleForAgents_${AWS::AccountId}
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - bedrock.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: BedrockInvokeModel
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:InvokeModel
                Resource: !Sub arn:aws:bedrock:${AWS::Region}::foundation-model/${BedrockModelId}
      
  ########################
  ##### ActionGroup #####
  ######################

  AgentLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.12
      FunctionName: !Sub biomarker-agent-${EnvironmentName}
      Handler: index.lambda_handler
      Role: !GetAtt AgentLambdaRole.Arn
      Timeout: 900
      Environment:
        Variables:
          BUCKET_NAME: !Ref S3Bucket

      Code:
        ZipFile: |
          import boto3
          import time
          import os
          import uuid
          import json
          import sys
          from collections import defaultdict
          redshift_client = boto3.client('redshift-data')

          def refineSQL(sql,question):
              schema = get_schema()
              
              prompt= f"""
              Here is the schema <schema>{json.dumps(schema)}</schema>
              Pay attention to the accepted values and the column data type located in the comment field for each column.

              Here is the generated sql query
              <sql>{sql}</sql>

              Here is the question that was asked 
              <question>{question}</question>
              
              <Instruction>Evaluate and refine the SQL query to make sure it is very efficient. If it is not efficient then respond back with a more efficient sql query, or respond with "no change needed" if the query is good. your response should with <efficientQuery></efficientQuery> tags </Instruction>
              <example>
              question: What is the survival status for patients who has undergone chemotherapy
              
              
              Inefficient SQL Query:
              SELECT chemotherapy, survival_status \nFROM dev.public.lung_cancer_cases\nWHERE chemotherapy = 'Yes';
              
              Reason: This is inefficient because it does not provide a more concise and informative output that directly answers the question. It results in a larger output size, does not aggregate the data, and presents the results in a difficult format that is not easy to analyze and interpret.
              
              This query will give you the count of patients for each survival status (Alive or Dead) who have undergone chemotherapy.
              
              The main differences are:
              
              1. It only selects the `survival_status` column, since that's the information needed to answer the question. The `chemotherapy` column is not needed in the output.
              
              2. It uses `COUNT(*)` and `GROUP BY` to aggregate and count the records for each distinct value of `survival_status`. This allows you to see the number of patients for each survival status in a single result set, rather than having to count them manually from the output.
              
              By aggregating the data using `COUNT` and `GROUP BY`, you can get a more concise and informative result, making it easier to analyze the survival status distribution for patients who have undergone chemotherapy.
              
              
              Efficient SQL Query:
              
              SELECT survival_status, COUNT(*) AS count
              FROM dev.public.lung_cancer_cases
              WHERE chemotherapy = 'Yes'
              GROUP BY survival_status;
              
              Reason: 
              This query will give you the count of patients for each survival status (Alive or Dead) who have undergone chemotherapy.
              
              The main differences are:
              
              1. It only selects the `survival_status` column, since that's the information needed to answer the question. The `chemotherapy` column is not needed in the output.
              
              2. It uses `COUNT(*)` and `GROUP BY` to aggregate and count the records for each distinct value of `survival_status`. This allows you to see the number of patients for each survival status in a single result set, rather than having to count them manually from the output.
              
              By aggregating the data using `COUNT` and `GROUP BY`, you can get a more concise and informative result, making it easier to analyze the survival status distribution for patients who have undergone chemotherapy.
              
          </example>
          <outputrule>do not use next line characters in the generated sql</outputrule>
              """
              client = boto3.client('bedrock-runtime')
              user_message= { "role": "user","content": prompt }
              claude_response = {"role": "assistant", "content": "<efficientQuery>"}
              model_Id='anthropic.claude-3-sonnet-20240229-v1:0'
              messages = [user_message,claude_response]
              system_prompt = "You are an extremly critical sql query evaluation assistant, your job is to look at the schema, sql query and question being asked to then evaluate the query to ensure it is efficient."
              max_tokens=1000
              
              body=json.dumps(
                      {
                          "messages": messages,
                          "anthropic_version": "bedrock-2023-05-31",
                          "max_tokens": max_tokens,
                          "system": system_prompt
                        
                      }  
                  )  
              
              response = client.invoke_model(body=body,modelId=model_Id)
              response_bytes = response.get("body").read()
              response_text = response_bytes.decode('utf-8')
              response_json = json.loads(response_text)
              content = response_json.get('content', [])
              for item in content:
                  if item.get('type') == 'text':
                      result_text = item.get('text')
                      print(result_text)
                      return result_text
              
              
              return "No SQL found in response"


          def get_schema():
              #Schema retrieval is hardcoded temporarily but also to make agent faster
              sql ="""
                    SELECT
                        'clinical_genomic' AS table_name,
                        a.attname AS column_name,
                        pg_catalog.format_type(a.atttypid, a.atttypmod) AS column_type,
                        pg_catalog.col_description(a.attrelid, a.attnum) AS column_comment
                    FROM
                        pg_catalog.pg_attribute a
                    WHERE
                        a.attrelid = 'clinical_genomic'::regclass
                        AND a.attnum > 0
                        AND NOT a.attisdropped
                    UNION ALL
                    SELECT
                        'chemotherapy_survival' AS table_name,
                        a.attname AS column_name,
                        pg_catalog.format_type(a.atttypid, a.atttypmod) AS column_type,
                        pg_catalog.col_description(a.attrelid, a.attnum) AS column_comment
                    FROM
                        pg_catalog.pg_attribute a
                    WHERE
                        a.attrelid = 'chemotherapy_survival'::regclass
                        AND a.attnum > 0
                        AND NOT a.attisdropped;"""
              
              try:
                  result = redshift_client.execute_statement(Database='dev', DbUser='admin', Sql=sql, ClusterIdentifier='biomarker-redshift-cluster')
                  print("SQL statement execution started. StatementId:", result['Id'])
              
                  def wait_for_query_completion(statement_id):
                      while True:
                          response = redshift_client.describe_statement(Id=statement_id)
                          status = response['Status']
                          if status == 'FINISHED':
                              print("SQL statement execution completed.")
                              break
                          elif status in ['FAILED', 'CANCELLED']:
                              print("SQL statement execution failed or was cancelled.")
                              break
                          print("Waiting for SQL statement execution to complete...")
                          time.sleep(5)  # Wait for 5 seconds before checking the status again
                  
                  wait_for_query_completion(result['Id'])
                  
                  # Retrieve the SQL result
                  response = redshift_client.get_statement_result(Id=result['Id'])
                  return response
              except Exception as e:
                  print("Error:", e)

          def query_redshift(query):
              try:
                  result = redshift_client.execute_statement(Database='dev', DbUser='admin', Sql=query, ClusterIdentifier='biomarker-redshift-cluster')
                  print("SQL statement execution started. StatementId:", result['Id'])
              
                  def wait_for_query_completion(statement_id):
                      while True:
                          response = redshift_client.describe_statement(Id=statement_id)
                          status = response['Status']
                          if status == 'FINISHED':
                              print("SQL statement execution completed.")
                              break
                          elif status in ['FAILED', 'CANCELLED']:
                              print("SQL statement execution failed or was cancelled.")
                              break
                          print("Waiting for SQL statement execution to complete...")
                          time.sleep(5)  # Wait for 5 seconds before checking the status again
                  
                  wait_for_query_completion(result['Id'])
                  
                  # Retrieve the SQL result
                  response = redshift_client.get_statement_result(Id=result['Id'])
                  return response
              except Exception as e:
                  print("Error:", e)

          # Clean response to send to model
          def extract_table_columns(query):
              table_columns = defaultdict(list)

              for record in query["Records"]:
                  table_name = record[0]["stringValue"]
                  column_name = record[1]["stringValue"]
                  table_columns[table_name].append(column_name)
              return dict(table_columns)

          # Upload result to S3 
          def upload_result_s3(result, bucket, key):
              s3 = boto3.resource('s3')
              s3object = s3.Object(bucket, key)
              s3object.put(
              Body=(bytes(json.dumps(result).encode('UTF-8')))
              )
              return(s3object)

          def lambda_handler(event, context):
            
              result = None
              
              if event['apiPath'] == "/getschema":
                  raw_schema = get_schema()
                  result = extract_table_columns(raw_schema)

              if event['apiPath'] == "/refineesql":
                  question = event['requestBody']['content']['application/json']['properties'][0]['value']
                  sql = event['requestBody']['content']['application/json']['properties'][1]['value']
                  result = refineSQL(question, sql)
              
              if event['apiPath'] == "/queryredshift":
                  query = event['requestBody']['content']['application/json']['properties'][0]['value']
                  result = query_redshift(query)

              # Example: Print or return the result
              if result:
                  print("Query Result:", result)
                
              else:
                  result="Query Failed."

              #Upload result to S3 if response size is too large
              BUCKET_NAME = os.environ['BUCKET_NAME']
              KEY= str(uuid.uuid4()) + '.json'
              size = sys.getsizeof(str(result))
              print(size)
              if size > 20000:
                  print('size greater than 20KB, hence writing to a file in S3')
                  result = upload_result_s3(result, BUCKET_NAME, KEY)

              
              response_body = {
              'application/json': {
                  'body':str(result)
              }
          }
    
              action_response = {
              'actionGroup': event['actionGroup'],
              'apiPath': event['apiPath'],
              'httpMethod': event['httpMethod'],
              'httpStatusCode': 200,
              'responseBody': response_body
              }

              session_attributes = event['sessionAttributes']
              prompt_session_attributes = event['promptSessionAttributes']
              
              api_response = {
                  'messageVersion': '1.0', 
                  'response': action_response,
                  'sessionAttributes': session_attributes,
                  'promptSessionAttributes': prompt_session_attributes
              }
                  
              return api_response


  AgentLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AmazonRedshiftDataFullAccess
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
      Policies:
        - PolicyName: RedshiftAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - redshift:*
                Resource:
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:*
        - PolicyName: S3Access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource: 
                  - !Sub arn:aws:s3:::${S3Bucket}/*
                  
                  
  AgentLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt AgentLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn

  ScientificPlotLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
      Policies:
        - PolicyName: S3PutObjectPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub 'arn:aws:s3:::${S3Bucket}/*'

  ScientificPlotLambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: EnsureECRImagePushed
    # DependsOn: TriggerBuildLambda
    Properties:
      FunctionName: ScientificPlotLambda
      PackageType: Image
      Code:
        ImageUri: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
      Role: !GetAtt ScientificPlotLambdaExecutionRole.Arn
      MemorySize: 512
      Timeout: 900
      Environment:
        Variables:
          S3_BUCKET: !Ref S3Bucket
  
  ScientificPlotLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt ScientificPlotLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn
  
  
  QueryPubMedLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess

  QueryPubMedLambdaFunction:
    Type: 'AWS::Lambda::Function'
    DependsOn: ScientificPlotLambdaFunction
    Properties:
      FunctionName: PubMedQueryFunction
      Handler: lambda_function.lambda_handler
      Role: !GetAtt QueryPubMedLambdaRole.Arn
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: pubmed-lambda-function.zip
      Runtime: python3.11
      Timeout: 30
      MemorySize: 128

  QueryPubMedLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt QueryPubMedLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn


  AgentIdSSMParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /streamlitapp/${EnvironmentName}/AGENT_ID
      Type: String
      Value: !GetAtt AgentResource.AgentId
      Description: !Sub SSM parameter for AgentId for ${EnvironmentName}

  AgentAliasIdSSMParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /streamlitapp/${EnvironmentName}/AGENT_ALIAS_ID
      Type: String
      Value: !GetAtt AgentAliasResource.AgentAliasId
      Description: !Sub SSM parameter for AgentAliasId for ${EnvironmentName}


  S3BucketNameSSMParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /streamlitapp/${EnvironmentName}/S3_BUCKET_NAME
      Type: String
      Value: !Ref S3Bucket
      Description: !Sub SSM parameter for S3 bucket name for ${EnvironmentName}


  
  enableCodeInterpreter:
    Type: Custom::enableCodeInterpreter
    DependsOn: 
      - AgentResource
    Properties:
      ServiceToken: !GetAtt CodeInterpreterLambda.Arn
      agentId: !GetAtt AgentResource.AgentId
      agentVersion: !GetAtt AgentResource.AgentVersion

  CodeInterpreterLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt CodeInterpreterLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import os

          def create_ssm_parameter(environment_name, agent_alias_id):
              ssm_client = boto3.client('ssm')
              parameter_name = f"/streamlitapp/{environment_name}/AGENT_ALIAS_ID"
              parameter_value = agent_alias_id
              parameter_type = "String"
              parameter_description = f"SSM parameter for AgentAliasId for {environment_name}"

              try:
                  ssm_client.put_parameter(
                      Name=parameter_name,
                      Value=parameter_value,
                      Type=parameter_type,
                      Description=parameter_description,
                      Overwrite=True
                  )
                  print(f"SSM Parameter '{parameter_name}' created/updated successfully.")
              except Exception as e:
                  print(f"Error creating/updating SSM Parameter: {str(e)}")
                  raise

          def get_ssm_parameter(environment_name):
              ssm_client = boto3.client('ssm')
              parameter_name = f"/streamlitapp/{environment_name}/AGENT_ALIAS_ID"
              
              try:
                  response = ssm_client.get_parameter(Name=parameter_name)
                  return response['Parameter']['Value']
              except ssm_client.exceptions.ParameterNotFound:
                  print(f"SSM Parameter '{parameter_name}' not found.")
                  return None
              except Exception as e:
                  print(f"Error retrieving SSM Parameter: {str(e)}")
                  raise

          def handler(event, context):
              client = boto3.client('bedrock-agent')
              environment_name = os.environ['ENVIRONMENT_NAME']
              
              try:
                  if event['RequestType'] in ['Create', 'Update']:
                      response = client.create_agent_action_group(
                          actionGroupName='CodeInterpreterAction',
                          actionGroupState='ENABLED',
                          agentId=os.environ['agentId'],
                          agentVersion=os.environ['agentVersion'],
                          parentActionGroupSignature='AMAZON.CodeInterpreter'
                      )
                      agent_id = os.environ['agentId']
                      agentPrep = client.prepare_agent(agentId=agent_id)
                      agentAlias = client.create_agent_alias(agentAliasName='v2', agentId=agent_id)
                      agent_alias_id = agentAlias['agentAliasId']
              
                      # Create SSM parameter
                      create_ssm_parameter(environment_name, agent_alias_id)
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Action group and SSM parameter created successfully"})
                  elif event['RequestType'] == 'Delete':
                      # Retrieve the agent alias ID from SSM
                      agent_alias_id = get_ssm_parameter(environment_name)
                      
                      if agent_alias_id:
                          # Delete the agent alias
                          response = client.delete_agent_alias(agentAliasId=agent_alias_id, agentId=os.environ['agentId'])
                          print(f"Agent alias {agent_alias_id} deleted successfully.")
                          
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Agent alias and SSM parameter deleted successfully"})
                      else:
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "No agent alias found to delete"})
                  else:
                      cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": "Unsupported request type"})
              except Exception as e:
                  print(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})

      Runtime: python3.12
      Timeout: 300
      Environment:
        Variables:
          agentId: !GetAtt AgentResource.AgentId
          agentVersion: !GetAtt AgentResource.AgentVersion
          ENVIRONMENT_NAME: !Ref EnvironmentName

  CodeInterpreterLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
      Policies:
        - PolicyName: CloudWatchLogsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
        - PolicyName: BedrockAgentActionGroupPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:CreateAgentActionGroup
                Resource: '*'
       
          

